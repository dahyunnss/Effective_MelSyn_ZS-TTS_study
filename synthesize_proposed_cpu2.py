import torch
import numpy as np
import os
import argparse
import librosa
import librosa.display
import re
import json
from string import punctuation
from g2p_en import G2p
import datetime
import matplotlib.pyplot as plt

from models_new_2conv.SCCNN_new_0206_cpu import SCCNN_Low, SCCNN_High
from hifigan.models import Generator as HiFiGAN
import hifigan
from text import text_to_sequence
import audio as Audio
import utils_cpu as utils
import time
from tqdm import tqdm
import traceback

import soundfile as sf
from pymcd.mcd import Calculate_MCD
from pesq import pesq


mcd_toolbox = Calculate_MCD(MCD_mode="plain")
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

def extract_filename(ref_audio):
    return os.path.splitext(os.path.basename(ref_audio))[0]

def wav_to_mel_spectrogram(wav_path, sr=16000, n_mels=80, n_fft=1024, hop_length=256):
    """
    WAV íŒŒì¼ì„ ë©œ ìŠ¤í™íŠ¸ë¡œê·¸ë¨ìœ¼ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.

    Parameters:
        wav_path (str): WAV íŒŒì¼ ê²½ë¡œ
        sr (int): ìƒ˜í”Œë§ ë ˆì´íŠ¸
        n_mels (int): ë©œ ë°´ë“œ ê°œìˆ˜
        n_fft (int): FFT í¬ê¸°
        hop_length (int): í™‰ í¬ê¸°

    Returns:
        np.ndarray: ë³€í™˜ëœ ë©œ ìŠ¤í™íŠ¸ë¡œê·¸ë¨
    """
    y, sr = librosa.load(wav_path, sr=sr)
    mel = librosa.feature.melspectrogram(y=y, sr=sr, n_mels=n_mels, n_fft=n_fft, hop_length=hop_length)
    mel_db = librosa.power_to_db(mel, ref=np.max)  # ë°ì‹œë²¨ ìŠ¤ì¼€ì¼ë¡œ ë³€í™˜
    return mel_db



def save_npy_with_original_name(wav_path, output_dir, data):
    """
    ì›ë³¸ WAV íŒŒì¼ê³¼ ë™ì¼í•œ ì´ë¦„ìœ¼ë¡œ NumPy ë°°ì—´ì„ ì €ì¥í•˜ëŠ” í•¨ìˆ˜.
    
    Parameters:
        wav_path (str): ì›ë³¸ WAV íŒŒì¼ ê²½ë¡œ
        output_dir (str): ì €ì¥í•  ë””ë ‰í† ë¦¬
        data (np.ndarray): ì €ì¥í•  NumPy ë°ì´í„°
    """
    base_name = extract_filename(wav_path)  # 'audio01'
    npy_path = os.path.join(output_dir, base_name)  # í™•ì¥ì ì—†ì´ ì €ì¥
    np.save(npy_path, data)  # 'audio01.npy'ë¡œ ì €ì¥ë¨
    print(f"[INFO] Saved: {npy_path}.npy")


def visualize_mel_spectrograms(base_name, save_path, original_wav_folder):
    """
    ìƒì„±ëœ ë©œ-ìŠ¤í™íŠ¸ë¡œê·¸ë¨ê³¼ ì›ë³¸ WAV íŒŒì¼ì„ ë¹„êµí•˜ì—¬ ì‹œê°í™”í•©ë‹ˆë‹¤.
    """
    # ğŸ”¹ ê²½ë¡œ ì„¤ì • (base_name ê¸°ì¤€)
    generated_npy_path = os.path.join(save_path, f"{base_name}.npy")
    output_image_path = os.path.join(save_path, f"{base_name}_mel_comparison.png")

    # ğŸ”¹ 1. `.npy` íŒŒì¼ ì¡´ì¬ ì—¬ë¶€ í™•ì¸
    if not os.path.exists(generated_npy_path):
        print(f"âŒ [ERROR] Generated mel-spectrogram file not found: {generated_npy_path}")
        return

    # ğŸ”¹ 2. ì›ë³¸ `WAV` íŒŒì¼ ì°¾ê¸° (í•˜ìœ„ í´ë”ê¹Œì§€ ê²€ìƒ‰)
    original_wav_path = None
    for root, _, files in os.walk(original_wav_folder):
        if f"{base_name}.wav" in files:
            original_wav_path = os.path.join(root, f"{base_name}.wav")
            break

    if original_wav_path is None:
        print(f"âŒ [ERROR] Original WAV file not found for {base_name} in {original_wav_folder}")
        return
    
    print(f"âœ… [INFO] Matched original_wav_path: {original_wav_path}")

    # ğŸ”¹ 3. `.npy` íŒŒì¼ ë¡œë“œ
    try:
        generated_mel = np.load(generated_npy_path)
        if generated_mel.ndim == 3:  # 3ì°¨ì› ë°°ì—´ì¸ ê²½ìš° squeeze
            generated_mel = np.squeeze(generated_mel)
        print(f"âœ… [INFO] Loaded Generated Mel Shape: {generated_mel.shape}")
    except Exception as e:
        print(f"âŒ [ERROR] Failed to load .npy file {generated_npy_path}: {e}")
        return

    # ğŸ”¹ 4. ì›ë³¸ WAV íŒŒì¼ì„ Mel-spectrogramìœ¼ë¡œ ë³€í™˜
    try:
        original_mel = wav_to_mel_spectrogram(original_wav_path)
        print(f"âœ… [INFO] Loaded Original Mel Shape: {original_mel.shape}")
    except Exception as e:
        print(f"âŒ [ERROR] Failed to process WAV file {original_wav_path}: {e}")
        return

    # ğŸ”¹ 5. ê¸¸ì´ ë§ì¶”ê¸°
    min_time_steps = min(generated_mel.shape[1], original_mel.shape[1])
    generated_mel = generated_mel[:, :min_time_steps]
    original_mel = original_mel[:, :min_time_steps]

    # ğŸ”¹ 6. ì‹œê°í™”
    fig, axes = plt.subplots(1, 2, figsize=(12, 6))

    axes[0].imshow(generated_mel, aspect='auto', origin='lower', cmap='viridis')
    axes[0].set_title("Generated mel-spectrogram")
    axes[0].set_xlabel("Time")
    axes[0].set_ylabel("Mel Frequency")

    axes[1].imshow(original_mel, aspect='auto', origin='lower', cmap='viridis')
    axes[1].set_title("Ground-truth mel-spectrogram")
    axes[1].set_xlabel("Time")

    plt.tight_layout()

    # ğŸ”¹ 7. ì´ë¯¸ì§€ ì €ì¥ í™•ì¸ í›„ ì €ì¥
    try:
        plt.savefig(output_image_path, dpi=300, bbox_inches='tight', pad_inches=0.1)
        print(f"âœ… [INFO] Comparison image saved: {output_image_path}")
    except Exception as e:
        print(f"âŒ [ERROR] Failed to save image {output_image_path}: {e}")

    plt.show()


def calculate_rtf(audio_length, processing_time):
    return processing_time / audio_length

def calculate_pesq(ref_wav_path, synth_wav_path, sr=16000):
    ref, _ = librosa.load(ref_wav_path, sr=sr)
    synth, _ = librosa.load(synth_wav_path, sr=sr)
    ref = np.asarray(ref * 32768, dtype=np.int16)  # Convert to int16 format
    synth = np.asarray(synth * 32768, dtype=np.int16)  # Convert to int16 format
    return pesq(sr, ref, synth, 'wb')

def create_log_file():
    timestamp = datetime.datetime.now().strftime("%Y%m%d-%H%M%S")
    log_filename = f"result_Libri100tr_test-clean_cpu_{timestamp}.txt"
    return log_filename


def log_data(log_filename, data):
    with open(log_filename, "a") as file:
        file.write(data + "\n")

def read_lexicon(lex_path):
    lexicon = {}
    with open(lex_path) as f:
        for line in f:
            temp = re.split(r"\s+", line.strip("\n"))
            word = temp[0]
            phones = temp[1:]
            if word.lower() not in lexicon:
                lexicon[word.lower()] = phones
    return lexicon


def preprocess_english(text, lexicon_path):
    text = text.rstrip(punctuation)
    lexicon = read_lexicon(lexicon_path)

    g2p = G2p()
    phones = []
    words = re.split(r"([,;.\-\?\!\s+])", text)
    for w in words:
        if w.lower() in lexicon:
            phones += lexicon[w.lower()]
        else:
            phones += list(filter(lambda p: p != " ", g2p(w)))
    phones = "{" + "}{".join(phones) + "}"
    phones = re.sub(r"\{[^\w\s]?\}", "{sp}", phones)
    phones = phones.replace("}{", " ")
    
    sequence = np.array(text_to_sequence(phones, ['english_cleaners']))
    
    return torch.from_numpy(sequence)


def preprocess_audio(audio_path, sampling_rate, _stft): 
    wav, sample_rate = librosa.load(audio_path, sr=None)
    if sample_rate != sampling_rate: #22050 != 16000
        wav = librosa.resample(wav, orig_sr=sample_rate, target_sr=sampling_rate)
    wav, _ = librosa.effects.trim(wav, top_db=20)
    mel_spectrogram, _ = Audio.tools.get_mel_from_wav(wav, _stft)
    log_mel_spectrogram = np.log(np.clip(mel_spectrogram, a_min=1e-10, a_max=None))
    return torch.from_numpy(log_mel_spectrogram)


def get_SCCNN_LO_HI(config, checkpoint_path, device):
    """Load both LO and HI SCCNN models from a single checkpoint"""
    # ğŸ”¥ ì²´í¬í¬ì¸íŠ¸ íŒŒì¼ í™•ì¸
    if not os.path.exists(checkpoint_path):
        raise FileNotFoundError(f"Checkpoint file not found: {checkpoint_path}")


    checkpoint = torch.load(checkpoint_path, map_location=device)

    #print(f"Checkpoint keys: {checkpoint.keys()}")
    model_lo = SCCNN_Low(config).to(device)
    model_hi = SCCNN_High(config).to(device)
    
    # ğŸ”¥ ëª¨ë¸ í‚¤ ì¡´ì¬ ì—¬ë¶€ í™•ì¸
    if 'model_lo' not in checkpoint or 'model_hi' not in checkpoint:
        raise KeyError(f"Checkpoint does not contain expected keys. Found keys: {checkpoint.keys()}")

    # ğŸ”¥ ëª¨ë¸ ìƒíƒœ ë”•ì…”ë„ˆë¦¬ í‚¤ í™•ì¸
    # print("Model LO keys:", model_lo.state_dict().keys())
    # print("Checkpoint LO keys:", checkpoint['model_lo'].keys())

    try:
        model_lo.load_state_dict(checkpoint['model_lo'])
        model_hi.load_state_dict(checkpoint['model_hi'])
    except RuntimeError as e:
        print(f"Error loading state_dict: {e}")
        model_lo.load_state_dict(checkpoint['model_lo'], strict=False)
        model_hi.load_state_dict(checkpoint['model_hi'], strict=False)


    model_lo.eval()
    model_hi.eval()

    return model_lo, model_hi

def get_vocoder(config, device):
    """
    HiFi-GAN Universal ëª¨ë¸(ë˜ëŠ” LJSpeech ëª¨ë¸ ë“±)ì„ ë¶ˆëŸ¬ì™€ì„œ
    PyTorch nn.Module í˜•íƒœë¡œ ë°˜í™˜í•˜ëŠ” í•¨ìˆ˜.
    """
    # (1) HiFi-GAN ì„¤ì • íŒŒì¼(hifigan/config.json) ë¶ˆëŸ¬ì˜¤ê¸°
    config_path = os.path.join("hifigan", "config.json")
    with open(config_path, "r") as f:
        hifigan_config = json.load(f)
        
    hifigan_config = hifigan.AttrDict(hifigan_config)
        
    # (2) HiFi-GAN Generator ì´ˆê¸°í™”
    vocoder = HiFiGAN(hifigan_config).to(device)
    
    # (3) Universalìš© ê°€ì¤‘ì¹˜ ë¡œë“œ (ë‹¤ë¥¸ ëª¨ë¸ì„ ì“°ë ¤ë©´ ê²½ë¡œ ë°”ê¾¸ê¸°)
    checkpoint_path = os.path.join("hifigan", "generator_universal.pth.tar")
    checkpoint_dict = torch.load(checkpoint_path, map_location=device)
    vocoder.load_state_dict(checkpoint_dict["generator"])
    vocoder.eval()
    vocoder.remove_weight_norm()  # weight_norm ì œê±° (inference ì‹œ í•„ìš”)

    return vocoder

def mel2wav(mel_np, vocoder, device):
    """
    mel: np.array or torch.Tensor
         shape = [T, n_mel_channels] í˜¹ì€ [n_mel_channels, T]
    vocoder: HiFi-GAN Generator (get_vocoderì—ì„œ ë¶ˆëŸ¬ì˜´)
    """
    
    if mel_np.ndim == 2:  # [80, T]
        mel = torch.from_numpy(mel_np).unsqueeze(0)  # => [1, 80, T]
    else:
        mel = torch.from_numpy(mel_np)  # ì´ë¯¸ ë°°ì¹˜ ì°¨ì› ìˆìœ¼ë©´ ê·¸ëŒ€ë¡œ ì‚¬ìš©
    
    mel = mel.to(device).float()
    with torch.no_grad():
        audio = vocoder(mel)  # => [1, audio_len]
    audio = audio.squeeze().cpu().numpy()
    return audio


    # if isinstance(mel, np.ndarray):
    #     mel = torch.from_numpy(mel)
        
    # mel = mel.unsqueeze(0).to(device).float()  # [1, 80, T]
    # if mel.dim() == 4:  # âŒ ì˜ëª»ëœ ì°¨ì›: [1, 1, 80, T]
    #     mel = mel.squeeze(1) 

    # with torch.no_grad():
    #     audio = vocoder(mel)
    # audio = audio.squeeze().cpu().numpy()  # -> [audio_length]
    # return audio



def synthesize(args, text, model_lo, model_hi, _stft, log_filename,device):  
    # print(f"Requested device: {device}")
    # print(f"CUDA available: {torch.cuda.is_available()}")
    # print(f"Current device: {torch.cuda.current_device() if torch.cuda.is_available() else 'CPU'}")
    
    save_path = args.save_path
    if not os.path.exists(save_path):
        os.makedirs(save_path, exist_ok=True)
    
    ref_audio_files = [os.path.join(root, f) for root, _, files in os.walk(args.ref_audio_dir) for f in files if f.endswith('.wav')]
   
    progress_bar = tqdm(total=len(ref_audio_files), desc="Processing batches", unit="batch", leave=True)

    all_mcd_scores = []
    all_pesq_scores = []
    all_inference_times = []
    all_rtf_scores = []

    for idx, ref_audio in enumerate(ref_audio_files):
        print(f"Processing batch {idx+1}/{len(ref_audio_files)}: {ref_audio}...")
        
        try:
            base_name = os.path.basename(ref_audio)  # íŒŒì¼ëª… (í™•ì¥ì í¬í•¨) ì¶”ì¶œ
            base_name = os.path.splitext(base_name)[0]  # í™•ì¥ì ì œê±°
            
            print(f"ğŸŸ¢ Extracted base_name: {base_name}")  # âœ… ì—¬ê¸°ì„œ ë””ë²„ê¹…

            # ê¸°ì¡´: base_nameì´ text ì•ˆì— í¬í•¨ëœ ë¬¸ìì—´ì„ ì°¾ìŒ
            # ë³€ê²½: í…ìŠ¤íŠ¸ íŒŒì¼ì´ "4992_41806_000032_000001|í…ìŠ¤íŠ¸" í˜•ì‹ì´ë¼ë©´, `split('|')[0]` ì‚¬ìš©
            text_entries = [line.split('|')[0] for line in text]  # âœ… base_nameë§Œ ì¶”ì¶œ

            matched_text = None
            for txt in text:
                if base_name == txt.split('|')[0]:  # âœ… ì •í™•í•œ ë¹„êµ
                    matched_text = txt
                    print(f"âœ… [INFO] Matched text found: {matched_text}")
                    break  # ì²« ë²ˆì§¸ ë§¤ì¹­ëœ í…ìŠ¤íŠ¸ ì‚¬ìš©

            if not matched_text:
                print("âŒ [ERROR] No matching text found! Printing first 5 text entries for debugging:")
                for i, line in enumerate(text[:5]):  # ì• 5ê°œë§Œ ì¶œë ¥
                    print(f"   {i+1}. {line}")
                raise ValueError(f"âŒ ERROR: No matching text found for {base_name}. Please check the input text file.")

            # 1) ë ˆí¼ëŸ°ìŠ¤ ë©œ ì¶”ì¶œ
            ref_mel = preprocess_audio(ref_audio, args.sampling_rate, _stft)
            ref_mel = ref_mel.transpose(0,1).unsqueeze(0)
            print("ref_mel", ref_mel.shape) #[1, 65, 40]
        
            # 2) ìŠ¤íƒ€ì¼ ë²¡í„°
            style_vector_lo = model_lo.get_style_vector(ref_mel).to(device)
            style_vector_hi = model_hi.get_style_vector(ref_mel).to(device)
            print(f"Style Vector Shapes - LO: {style_vector_lo.shape}, HI: {style_vector_hi.shape}")
            
            mcd_scores = []
            pesq_scores = []
            inference_times = []
            rtf_scores = []

            # 3) í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬
            # Forward
            src = preprocess_english(matched_text, args.lexicon_path).unsqueeze(0).to(device)
            src_len = torch.tensor([src.shape[1]], device=device)
            print(f"ğŸ“ Text input shape: {src.shape}, src_len: {src_len}")

            # 4) ëª¨ë¸ ì¶”ë¡ 
            cpu_start = time.process_time()
            with torch.no_grad():
                raw_lo = model_lo.inference(style_vector_lo, src, src_len)
                raw_hi = model_hi.inference(style_vector_hi, src, src_len)

            cpu_end = time.process_time()
            inf_time = cpu_end - cpu_start
            inference_times.append(inf_time)
            
            mel_output_lo = raw_lo[0]
            mel_output_hi = raw_hi[0]
            print(f"mel_output_lo shape: {mel_output_lo.shape}, mel_output_hi shape: {mel_output_hi.shape}")
            
            # 5) ì‹œê°„ ì¶• ë§ì¶° íŒ¨ë”©(0ë²ˆ ì¶•ì´ ì‹œê°„)
            T_lo = mel_output_lo.shape[1]
            T_hi = mel_output_hi.shape[1]
            max_T = max(T_lo, T_hi)
            
            # timeì¶•(dim=1)ìœ¼ë¡œ íŒ¨ë”©
            mel_output_lo = torch.nn.functional.pad(
                mel_output_lo, (0, 0, 0, max_T - T_lo)
            )  # => [1, max_T, 40]
            mel_output_hi = torch.nn.functional.pad(
                mel_output_hi, (0, 0, 0, max_T - T_hi)
            )
            # 6) ëª¨ë©˜íŠ¸ ë§¤ì¹­(ì •ê·œí™”)
            # lo_mean, lo_std = mel_output_lo.mean(), mel_output_lo.std()
            # hi_mean, hi_std = mel_output_hi.mean(), mel_output_hi.std()
            # print(f"Before normalization: LO mean = {lo_mean:.4f}, LO std = {lo_std:.4f}; HI mean = {hi_mean:.4f}, HI std = {hi_std:.4f}")
            
            # mel_output_hi = (mel_output_hi - hi_mean) * (lo_std / (hi_std + 1e-5)) + lo_mean
            # print(f"After normalization: HI mean = {mel_output_hi.mean():.4f}, HI std = {mel_output_hi.std():.4f}")

            # freqì¶•(dim=2)ìœ¼ë¡œ concat => [1, max_T, 80]
            mel_output = torch.cat([mel_output_lo, mel_output_hi], dim=2)  # shape: [1, T, 80]
            print(f"[DEBUG] Final mel_output shape (time, freq=80): {mel_output.shape}")
            
            # 9) HiFiâ€GANì— ë§ì¶° [1, 80, max_T]ë¡œ permute(ì €ì¥, ì‹œê°í™”ë¥¼ ìœ„í•¨)
            mel_output = mel_output.permute(0, 2, 1)
            print("Final mel_output for vocoder:", mel_output.shape)  # [1, 80, max_T]

            # 10) mel_spectrogram ì €ì¥
            wav_save_path = os.path.join(save_path, f"{base_name}.wav")
            npy_save_path = os.path.join(save_path, f"{base_name}.npy")
            np.save(npy_save_path, mel_output.cpu().numpy()) 
            print(f"ğŸ“ npy_save_path: {npy_save_path}") 
            
            img_save_path = os.path.join(save_path, f"{base_name}_mel_comparison.png")
            print(f"ğŸ“ img_save_path: {img_save_path}")
            
            # ì‹œê°í™”
            if os.path.exists(npy_save_path):
                visualize_mel_spectrograms(base_name, save_path, args.ref_audio_dir)
            else:
                print(f"âŒ [ERROR] .npy file not found: {npy_save_path}")

                
            # 11) Vocoderë¡œë¶€í„° íŒŒí˜• ë³µì›
            #    mel2wav ì•ˆì—ì„œ [1, 80, T] í˜•íƒœë¡œ ë³€í™˜í•˜ë„ë¡ ìˆ˜ì •
            #wav_output = mel2wav(mel_output.cpu().numpy(), vocoder, device)
            wav_output = mel2wav(mel_output.squeeze(0).cpu().numpy(), vocoder, device)

            # âœ… WAV íŒŒì¼ ì €ì¥ (base_name ìœ ì§€)
            sf.write(wav_save_path, wav_output, args.sampling_rate)
            print(f"[INFO] Saved waveform: {wav_save_path}")
            #7176_92135_000040_000001_260_123440.wav

            # Calculate RTF
            audio_length = len(wav_output) / args.sampling_rate
            rtf = calculate_rtf(audio_length, inf_time)
            rtf_scores.append(rtf)
            
            # Calculate MCD and PESQ
            synthesized_wav_path = wav_save_path
            reference_wav_path = ref_audio
            
            mcd_value = mcd_toolbox.calculate_mcd(reference_wav_path, synthesized_wav_path)
            mcd_scores.append(mcd_value)

            pesq_value = calculate_pesq(reference_wav_path, synthesized_wav_path, args.sampling_rate)
            pesq_scores.append(pesq_value)
    
            log_data(log_filename, f"{txt.split('|')[0]}: MCD Score = {mcd_value:}, PESQ Score = {pesq_value:}, RTF = {rtf:}")
            log_data(log_filename, f"Inference Time: {inf_time:} seconds")

            all_mcd_scores.append(mcd_value)
            all_pesq_scores.append(pesq_value)
            all_inference_times.append(inf_time)
            all_rtf_scores.append(calculate_rtf(len(wav_output) / args.sampling_rate, inf_time))

            progress_bar.update(1)
            
        except Exception as e:
            print(f"âŒ Error processing {ref_audio}: {e}")
            traceback.print_exc()
            break
            
    if all_mcd_scores or all_pesq_scores or all_inference_times or all_rtf_scores:
        average_mcd = sum(all_mcd_scores) / len(all_mcd_scores) if all_mcd_scores else 0
        average_pesq = sum(all_pesq_scores) / len(all_pesq_scores) if all_pesq_scores else 0
        avg_inf_time = sum(all_inference_times) / len(all_inference_times) if all_inference_times else 0
        average_rtf = sum(all_rtf_scores) / len(all_rtf_scores) if all_rtf_scores else 0

        log_data(log_filename, f"Average MCD Score: {average_mcd:.7f}, Average PESQ Score: {average_pesq:.7f}, Average RTF: {average_rtf:.7f}")
        log_data(log_filename, f"Average Inference Time:: {avg_inf_time:.7f} seconds")
 
        print('âœ… All processing done!')

    progress_bar.close()

if __name__ == "__main__":
    from audio.stft import TacotronSTFT
    parser = argparse.ArgumentParser()
    parser.add_argument("--checkpoint_path", type=str, required=True, 
        help="Path to the pretrained model")
    parser.add_argument('--config', default='configs/config2.json')
    parser.add_argument("--save_path", type=str)
    parser.add_argument("--ref_audio_dir", type=str, required=True,
        help="path to the directory containing reference speech audio samples")
    parser.add_argument("--text_file", type=str, required=True,
        help="path to the text file containing text to synthesize")
    parser.add_argument("--lexicon_path", type=str, default='lexicon/librispeech-lexicon.txt')
    parser.add_argument(
        "--device",
        type=str,
        choices=["cpu", "cuda"],
        default="cpu",
        help="Device to run the model on (cpu or cuda)",
    )
    args = parser.parse_args()
    device = torch.device(args.device)
    
    with open(args.config) as f:
        data = f.read()
    json_config = json.loads(data)
    config = utils.AttrDict(json_config)

    # Get model
    model_lo, model_hi = get_SCCNN_LO_HI(config, args.checkpoint_path, device)
    print(f"Model lo device: {next(model_lo.parameters()).device}")
    print(f"Model hi device: {next(model_hi.parameters()).device}")
 
    # Load vocoder
    vocoder = get_vocoder(config, device)

    _stft = Audio.stft.TacotronSTFT(
                config.filter_length,
                config.hop_length,
                config.win_length,
                config.n_mel_channels,
                config.sampling_rate,
                config.mel_fmin,
                config.mel_fmax)

    log_filename = create_log_file()
    print(f"Log file created: {log_filename}")
    
    print(f"Reading text from {args.text_file}")
    with open(args.text_file, 'r') as file:
        args.text = file.readlines()
        args.text = [line.strip() for line in args.text if line.strip()]  

    args.sampling_rate = config.sampling_rate
    print("Starting synthesis...")
    synthesize(args, args.text, model_lo, model_hi, _stft, log_filename,device)
